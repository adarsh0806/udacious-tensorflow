{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Logistic Classifier\n",
    "\n",
    "It is a linear classifier(applies a linear function on the input to make predictions). Linear function is a matrix multiplier. \n",
    "\n",
    "`WX + b = Y`\n",
    "\n",
    "* X - inputs\n",
    "* W - weights\n",
    "* b - bias term\n",
    "\n",
    "ML trains on the weights and bias.\n",
    "\n",
    "* Increase the size of the outputs(Y * 10), classifier becomes very sure of the predictor\n",
    "* Decrease the size of the outputs(Y / 10), classifier becomes very unsure of the predictor\n",
    "* Need to find the right balance here.\n",
    "\n",
    "\n",
    "Backprop:\n",
    "* https://classroom.udacity.com/nanodegrees/nd009/parts/bffb699a-f15b-48e6-bf92-574645336846/modules/24611a52-b490-497c-aaba-47db1f4ef3fd/lessons/6379031992/concepts/64370585660923\n",
    "\n",
    "\n",
    "Regularization:\n",
    "* Applying artificial constraints on your network that implicitly reduce the number of free parameters while not making it more difficult to optimize. \n",
    "* Stretch pants of regularization - L2 regularization - add a term to the loss that penalizes large weights. This term is the L2 norm of the weights multiplied by a small constant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN\n",
    "\n",
    "** Statistical Invariance **\n",
    "https://classroom.udacity.com/nanodegrees/nd009/parts/bffb699a-f15b-48e6-bf92-574645336846/modules/4ea86e84-80cc-4e4c-999f-a34e07d03bef/lessons/6377263405/concepts/63796332430923\n",
    "\n",
    "\n",
    "* Lets say we're trying to find the image with a cat in it. It doesn't matter where the cat is.\n",
    "* Instead of trying to learn about kittens in the top left and bottom right corner of the image independantly, you can tell the model that the cat pictures are the same irrespective of WHERE they occur in the overall image.\n",
    "* This is called translation invariance.\n",
    "* Similarly, if our model finds the word kitten in different portions of text, the word is still the same, so the model shouldnt have to re learn the meaning of the word once its already learnt. \n",
    "* This can be achieved by Weight Sharing. \n",
    "* Images - CNN\n",
    "* Text, sequences in general - RNNs, Embeddings\n",
    "\n",
    "** ConvNets **\n",
    "\n",
    "* Uses shared parameters across space to extract patterns in an image. \n",
    "* NNs that share their parameters across space.\n",
    "* Stacks of convolutions forms a pyramid.\n",
    "* The convolution stack progressively squeezes the dimensions(spatial information is squeezed out), while increasing the depth. \n",
    "* Patch/Kernel, each pancake in the stack is called a feature map.\n",
    "* Stride - number of pixels being shifted each time the filter is moved(when stride = 1, the output is roughly the same size as the input, stride of 2, output is roughly half the size of the filter)\n",
    "* You can use padding to ensure the output and input sizes match(same padding)\n",
    "\n",
    "ConvNets improvements:\n",
    "\n",
    "* ** Pooling** - better way to reduce the spatial extent of the feature maps in the convolutional pyramid. Pooling refers to combining convolutions in a neighborhood. Max pooling - for every point on the feature map, look at the neighborhood around that point, and compute the maximum of all the reponses around it. Average pooling\n",
    "* **1 by 1 Convolutions** - dont look at the patch of image, looks at just the one pixel. Classic ConvNets are a small classifier for a patch of the image, but its a linear classifier. If you add a 1 by 1 ConvNet in the middle of the ConvNet, we get a mini NN running over the patch instead of linear classifier. \n",
    "* **Inception module** -  instead of having a single convolution you have average pooling followed by 1by1 convolution, then  a 1by1 convolution, then 1by1 followed by a 3by3, then a 1by1 followed by a 5by5. At the top concatenate the output of each of them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Models for Text and Sequences ##\n",
    "\n",
    "Embeddings: \n",
    "\n",
    "* Similar words occur in similar contexts. We can use embeddings to map words to small vectors. Word vectors with similar meanings will be close to each other and different meanings will be far apart.\n",
    "* [Word2Vec](https://classroom.udacity.com/nanodegrees/nd009/parts/bffb699a-f15b-48e6-bf92-574645336846/modules/2a61e668-53aa-49a3-b5d2-03532ee92760/lessons/6378983156/concepts/63741135970923)\n",
    "\n",
    "\n",
    "** [RNNs](https://classroom.udacity.com/nanodegrees/nd009/parts/bffb699a-f15b-48e6-bf92-574645336846/modules/2a61e668-53aa-49a3-b5d2-03532ee92760/lessons/6378983156/concepts/63770919610923) **\n",
    "\n",
    "* Uses shared parameters across time to extract patterns in text.\n",
    "* Backpropagation occurs all the way back to the beginning of the first event that is being recorded. The derivatives are applied to the same parameters(correlated updates happen to the same weights). This is bad for SGD leading to the exploding gradient and vanishing gradient problem.\n",
    "* Exploding gradient is solved by ** Gradient Clipping **. In order to prevent the gradient from growing unbounded, compute the norm and shrink the step when the norm goes too big. Vanishing gradients make your model remember only recent events, solved by using ** LSTMs **.\n",
    "\n",
    "** [LSTM](https://classroom.udacity.com/nanodegrees/nd009/parts/bffb699a-f15b-48e6-bf92-574645336846/modules/2a61e668-53aa-49a3-b5d2-03532ee92760/lessons/6378983156/concepts/63789220230923) **\n",
    "\n",
    "* Take a simple model of memory, replace everything with continuous functions and replace that the NN machine thats at the heart of the NN\n",
    "* There is read node, memory node, forget node and forget node. \n",
    "* The gates are not binary, they have logistic regression values which are calculated using the input weights. \n",
    "* Regularization - L2, Dropout(only on the input and output and not the recurrent connections)\n",
    "* Application - Beam search(word recommendations when typing out a word in search bar, the correct word is chosen based on a sampling form distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
