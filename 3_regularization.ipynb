{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "## Assignment 3 - Regularization\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization on a logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 28.001463\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.3%\n",
      "Minibatch loss at step 500: 2.525669\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1000: 1.376055\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1500: 0.705039\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 0.659546\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 0.658754\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.790505\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Setting up of the TensorFlow graph\n",
    "The beta constant value is set to a small value and multiplied with the L2 norm of the weight to penalize large values.\n",
    "'''\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    # For training data we use a placeholder that will be fed at runtime\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape = (batch_size, image_size * image_size)) # 128, 28*28\n",
    "    tf_train_labels = tf.placeholder(tf.float32,\n",
    "                                     shape = (batch_size, num_labels)) # 128, 10\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_constant = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables - weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels])) # 28*28, 10\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "                beta_constant * tf.nn.l2_loss(weights) # using the 0.002 value for the beta constant\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    \n",
    "    \n",
    "        \n",
    "'''\n",
    "Running TensorFlow Logistic Model\n",
    "'''   \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in xrange(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels,\n",
    "                     beta_constant : 0.002} # hard coding beta value\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, \n",
    "                                         loss, \n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):           \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right value for the beta parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating over the following beta values: \n",
      "\n",
      " [0.0001, 0.00012589254117941674, 0.00015848931924611142, 0.00019952623149688809, 0.00025118864315095823, 0.00031622776601683826, 0.00039810717055349773, 0.00050118723362727296, 0.00063095734448019429, 0.00079432823472428294, 0.001000000000000002, 0.0012589254117941701, 0.0015848931924611173, 0.001995262314968885, 0.0025118864315095872, 0.0031622776601683889, 0.0039810717055349856, 0.0050118723362727402, 0.0063095734448019554, 0.0079432823472428467]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run the TF model on a range of Beta values\n",
    "'''\n",
    "num_steps = 3001\n",
    "# Beta values to iterate over\n",
    "beta_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "print(\"Iterating over the following beta values: \\n\\n\",regul_val)\n",
    "\n",
    "for beta in beta_val:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels, \n",
    "                         beta_constant : beta}\n",
    "            \n",
    "            _, l, predictions = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        # Append all the accuracy scores for the corresponding beta values.\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['f', 'beta', 'step']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns3p8/l5RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x144840410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plotting the test accuracy by beta value\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "plt.semilogx(beta_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by beta regularization value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization using a 1 layer neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set up TF graph\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_constant = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # First hidden layer\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    hidden_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    \n",
    "    # Final layer\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(hidden_1, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \\\n",
    "                + beta_constant * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    validation_prediction = tf.nn.softmax(tf.matmul(layer1_validation, weights) + biases)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 386.652130\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.2%\n",
      "Minibatch loss at step 500: 54.191212\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 35.871338\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1500: 29.173382\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 28.224617\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 24.483812\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 3000: 24.411398\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run TF graph\n",
    "'''\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels, \n",
    "                     beta_constant : 1e-4}\n",
    "            \n",
    "        _, l, predictions = session.run([optimizer, \n",
    "                                         loss, \n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(validation_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right value for the beta parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step in.\n",
      "Step in.\n",
      "Step in.\n"
     ]
    }
   ],
   "source": [
    "# num_steps = 3001\n",
    "num_steps = 30\n",
    "# beta_values = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "beta_values = [0.003, 0.002, 0.001]\n",
    "accuracy_val = []\n",
    "\n",
    "for beta in beta_values:    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels, \n",
    "                         beta_constant : beta}\n",
    "            _, l, predictions = session.run([optimizer, \n",
    "                                             loss, \n",
    "                                             train_prediction], feed_dict=feed_dict)\n",
    "        # Get accuracy scores\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        print(\"Step in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEOCAYAAACD5gx6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8VXXVx/HPYh4UrwiJiHAVJxzwiimE4XMTDZzASjMo\nFSqHnhweMlMrRcvMCYcsLYfCzCmxQdAcSI9DmkOCiAIiKggIoggypEzr+eO3LxyO995zLmfY5+z7\nfb9evDi/Pa4z3HX2Xvu3f8fcHRERSZ4WcQcgIiLFoQQvIpJQSvAiIgmlBC8iklBK8CIiCaUELyKS\nUErwUhBm1tbMNphZ97hjaSoze87MRuax/ptm1r/AMbUxsxVm1q2Q203b/jVmdmoxtr0lzGwPM1sb\n074nmdkhcey72JpNgo/+WD6O/q03s9Vp00bksd28kkPCNMubKtx9V3d/Pp9tZH6O3H2Nu2/t7ovy\nj/Az++oOfA34fdTuYGb3m9k70Zf0QVnW/7eZrTSzz6VNO8rMZqS1F5nZu2bWJm3a983sH41sOq7P\nzxXAZTHtu6iaTYKP/lg6uXsnYC5wVNq0u+OOr1jMrGUpd1eUjZb2OeSsXOPKwbeBv7n7uqjtwBPA\nN4ClOazvwH+BH9czPf1xO+D7jSwTKzMzAHd/GuhhZnvHHFLBNZsEn8HISEZm1sLMLjSzOWb2vpnd\nYWadonkdzOxuM/vQzD6Kjra2MbOrgQOBW6Mzgas+syOzlmY2ITqiWWpm/zSz3dPmdzCzX5nZvGjb\nT5hZi2hebbSvZdHR1Tei6Zsd7ZnZaWb2WPS4rlRyupm9CbwaTb8xOqJaHh2B9c+IcWz03Jeb2fNm\n9jkzu9XMLs14Po+Y2WmNvLZfMbO3zWxx3bpm1j7abu+07fQws1V1r3HGPk6LXqdfm9lS4Ly06TPN\n7AMzm5heDoqOIN+IXuNr018jM/ulmd2ctmyD5YBo3hPRe73YzMabWce0+e+Z2TlmNh1YnjZtYPQZ\nSj9TXBm9F58zsy5m9lD02frAzP5mZttH63/mc2QZJS8z29bM7orWn2Nm52a8XpPN7ProMzTbzAY3\n8h4dATxZ13D3/7r7r9393+SegK8DRptZj0aWuRI4z8w65LjNjczsVDObEb0eb5jZ6LR5mz2/6LVa\nZmZ7RO1B0Wf8IzN7ycwGpi37nJldYmb/BlalfYaeAo5sapzlrrkm+PqcCxwGDAR6AGuBa6N53wVa\nAjsA2wFnAGvc/YfAi8B3ojOBcz+z1eBvwM5AN2AmcHvavBuA3YEDgM7ATwE3s12BiYTTx87R/Nca\niT/zD/MooB+wf9R+Ftg72tbfgfts0xHoj4FhwGHuvg1wKvBJFGf6F8kOwMHAvY3EcTTQFzgIGGFm\nI939v8B9wLfSlhsJTHL3jxvYziDgZcLrPc7MTgDOip7X9sAU4E9pcd0DnA10BRZGz70xjSWyS4DP\nAfsS3pufZMz/OjA4im3TBt03ZJwp/g54DFhC+Fu7ifDZ2jna/7XReg19jtJj/B3hM9gL+DLwPdu8\ntDgIeIHw/v4GuLWR57cvMKuR+bl4G7gDGNvIMv8CXgLGbMH2FwJDotfxdOA3ZtYnmnc7cGLasscC\ns9x9lplVA38FLnD3bQl/T38zs23Slv8m4bO4NVBXApsB7LcFcZY3d292/wgfzkMzpr0FfCGtvTOw\nKnr8PcIp7N71bOs5YGQT9t0NWA+0AVoBa4Bd61nuYuDOBrax2T6B04BHo8dtgQ1A/0ZiMGAVsFvU\nfoeQ3Otb9k3g4OjxOcCEBpar2++gtGljgInR40OA2WnzpgFHN7Ct04CZGdMeB0aktVtHr11X4BTg\nnxnPb3HdawT8Erg5bf4ehC/orO8hcALwr7T2e8AJGcu8BwzMmHYS8AawTQPbHQAsaOQ9rXs9u0ef\nlXVAr7T5ZwEPpb1e09LmbRt9xjrVs98W0XZ7NhDXEuCgLJ/h5whf0DsQzmJ6E754X898TQgHGB8A\nnQjlmoca2OZm70k98/8BnBI97gUsA9pG7YnA96PHFwG/y1g3BRyfFvv59Wz/DMIBR+z5qZD/dAS/\nyU7AQ9Ep/lLC0SNm1hm4jXAKN8FCKeUXZpZTvTkqf4yLTquXEY4UIBz97UA4KnurgXjm5PF85mfE\ncUFU3viIUGdtC3SJZu/YQAwQjtLqjry/FbVz3e9cQoLC3Z8CWphZfzPbj/BF19gFt3cz2r2A36a9\nP+8TEnyPaB8bl/fwF7sgS5z1MrMdzOzPZjY/er9uZdPrVGd+Paumb6M/cBUwzN3ryjhbmdltZjY3\n2u4j9Wy3Id0IX1rpr8lcwvtWJ/1i7Opo+a0yN+TuG4CPCUeveXH394CbCWc8DS0zhXBw9KOmbNvM\nhlkoFX4YfWa/RPR6uftcwt/nsWbWBTiUcAYH4XNyYt3nJFr3AMLfWp3MzxaE12NZU2KsBErwm8wn\nHNV3jv5t6+4d3X2phx4NY929D+FI9HjCBSnIXrMcTTid/x93rwL2jKYb4ShnHeEIKNO7wK4NbHMV\nkF7XrK8r3ca4zOwwwhHKcA+nrZ0JJZi6L6n5DcQA8EfgODPrR0imDzawXJ2d0h73JJxqp2/rxOjf\nPe6+vpHtZL6u84BRGe/PVlECeS99v9GXb3ryy3y90v/YM10FrAT2it6v7/LZi8cNvudRTXcCodwy\nM23W+VFMB0Tb/XLGdhv7HC0iOupOm9aTLfwSI1yX2T3rUrm5glCW26eRZcYSPn+fa2SZjaKa/Z8J\nXxxdos/sE2z+etV9lr5BOHv7MJr+LnBLxudka3f/Vdq69b3WfYBXcomvkijBb/I74Iq6i0bRhbGj\no8eDzaxPlDhWEpJyXXJaDOzSyHa3JiTTj8xsK+AXdTM89GL4I3B9tL8WZnZwtJ87gKPMbHh0FtDF\nzPaNVp1KSLptzWxPYFSW57Y14Wj3QzNrC/yccARf5zbgMjPbOXq+NRZd/HT3twlnHX8A7vVNPS8a\ncp6ZdYpqoWew6ciK6Dl9nfBH+ccs28n0O+BCiy5QRxcdvxrNewA4yMyGRtcVzgGq0tadCnzJzLqb\n2bY0fjS5NeE9XmlmPYEf5BqgmbUG/gL81t0n1bPd1cDH0VHnTzPmN/g5cvc1hLryZRYuyvcmXG/I\ndjbVkIeA2ozY25hZu6jZNvqcZOXuHwDXE65hNbTM64T3KLNHTaa6BN6eUL5cEsU2LDNewpfoFwn1\n+fTP0u3A8WZ2aPT31D56nO3L5RAaP6OsSM01wdf3DX4F4YLY42a2HHiGTRcodyRcmPyYUDue5O5/\njuZdC5wcnUpeXs92byPUIBcRjhCeyph/NqEUMyVa7meAufscYDjhAt9SwkW4vaJ1riTUoN8Hfstn\n/9Azn99E4OloP29G6y1Jm3854ci87rnfxOZfALcTjtCyJWWPtvMK4YLfve5+58aZ7m8RLu6tcPeX\nsmxr8w2730O4IP2XqMTxMuGiOB76io+I5i8hlGxeBT6NVn8QmAS8TrjY/Nd64q5zEeGC5TLgfkIi\naWjZzGm7EHrDnBf1/qjrUdMFuJpwveBDwmcg80yovs9R+r5OJyTAucBkwjWFxrr3NnZGMB4Ybmat\n0qbNJZzpdCbUrFc3khQztz2unumZy1xM+JJrLC4HiI7Gf0h4zz4gdADY7PVy95WEz3V3wpdH3fS3\nCX38L4nWfZtwvaIu131m/2Y2CJjv7tMbia0iWXSBofGFzMYA3yGcJr5K6Ed7PuHi1vvRYj9294eL\nFKfEyMwOB37j7nmf1pvZncBr7l60G0uio/hFhIu4ed2AlFQWuma+4e43Z124TJnZL4Cu7p7XHblm\n9gBwjbunChJYGcma4KOa4jPAnu6+xszuJZziVROOxK4pepQSGwt3It4PpNx9XLbls2xrV0K3uT7R\nBbqCMbOhhKPzNYSznhMJvZOylZSkAplZV0LpbXhTzwabk1xLNC2BjtEpXQc2Xdwpyp2LUh6i3i5L\ngY7AjXlu6wrgP8AlhU7ukUMIp+OLCD0uvqLknkxm9n3Ce32vknvjci3RnEW4OLia0N/6RDMbS7i4\nt5xwVHZOXZcwERGJXy4lmirCKfrxhGQ+gXBX4mPAB+7uFm5J38Hdv1PP+mUz9oSISCVx97yqJLmU\naA4D3or6g68ndAMb6O5LfNO3wy2E3gMNBal/ZfRv7Nixsceg51pecZZq/8XcTyG3XYht5buNQsgl\nwc8DBphZu6h/9mBghm0+TvVXgcR1MUqq2trauEMomUp5rnHHWar9F3M/hdx2IbYV93sKudfgxxJu\nTllL6H98CqF/dw2h6+Q7wGnuvriedb1Q30YiIs2FmeF5lmhySvB57UAJXkSkyQqR4JvrnawiIomn\nBC8iklBK8CIiCaUELyKSUErwIiIJpQQvIpJQSvAiIgmlBC8iklBK8CIiCaUELyKSUErwIiIJpQQv\nIpJQSvAiIgmlBC8iklBK8CIiCaUELyKSUErwzcj69fDnP8OKFXFHIiKloATfTDz5JPTrB6NHw913\nxx2NiJSCfrIv4ebOhXPPheefh6uvDkfxf/oTTJoUd2Qi0piS/WSfmY0xs+lmNs3M7jSzNmnzzjGz\nDWbWOZ9ApLBWr4aLL4YDDoB99oEZM+D44+GII+Cpp2DlyrgjFJFiy5rgzaw7cCbQz937Aq2Ab0Tz\negCHA3OLGaTkzh3uvRf69IGZM+Hll+Gii6BDhzB/m21gwAB49NF44xSR4su1Bt8S6GhmrYAOwMJo\n+rXAucUITJpu6lSorYXLL4c77oB77oGePT+73LBh8MADJQ9PREosa4J394XAOGAesABY5u6TzWw4\n8K67v1rkGCWLDz6A00+HoUPhm9+El16CQw5pePlhw+DBB2HdutLFKCKl1yrbAmZWBQwHegHLgfvM\n7ETg+4TyzMZFG9rGqFGjqK6uBqCqqoqamhpqa2sBSKVSAGpvQXvtWhgzJsUdd8Do0bXMmAGvvJLi\n6aezr7/TTrU8+yxs2FA+z0dttZtzO5VKMX78eICN+TJfWXvRmNlxwBB3PyVqnwiMBvYCVhMSew/C\n0f1B7v5+xvrqRVMEjz0G//d/sOOOcN11sNdeTVv/4ovDhdarry5KeCKSp1L1opkHDDCzdmZmwGDg\nfnfv5u67uPvOwHxg/8zkLoU3Zw4ce2woyVx2GTzySNOTO8Dw4fD3v4eLsiKSTLnU4F8AJgBTgFcI\nR+w3Zy5GIyUayd/KlXDBBdC/f+gF8/rrIUnbFr7qNTXw6aehp42IJFNOvWjc/RJ37+Pufd39ZHdf\nmzF/F3dfWpwQm7cNG0KPmD33hAULYNo0OP98aNs2v+2ahYutf/97YeIUkfKjoQrK2IsvwsEHww03\nwIQJ8Mc/Qvfuhdt+XZlGRJJJQxWUoUWLQjnmkUfgF7+Ak0+GFkX4Kl6zBrbfPtzl2q1b4bcvIluu\nZEMVSGmsWQNXXRWGFujaNdTHR48uTnIHaNMGhgzRuDQiSaUEXyYefDAk9iefhGefhSuvhE6dir9f\nlWlEkkslmpjNmgVjxsBbb8G114bBwEpp2bIwnMF770HHjqXdt4g0TCWaCrZ8OZxzDnzxi3DYYaF3\nTKmTO0BVFRx0ULhxSkSSRQm+xDZsgNtuC90ely+H6dPhBz8I9fC4aPAxkWRSiaaEnn0Wzjor9GH/\n1a/CWO3lYO5cOPDAUKZp2TLuaEQEVKKpGPPnh1EeTzghHK0/80z5JHeAXr1C//rnnos7EhEpJCX4\nIvrkk9CPvaYGdt459DcfOXLLhxcoJpVpRJJHCb4I3OGvfw2DgL38MrzwAlx6KWy1VdyRNUzdJUWS\nJ+t48NI006eHYXwXLYJbboHBg+OOKDf9+sGqVeHmqj33jDsaESkEHcEXyNKlcOaZcOihYTjfqVMr\nJ7nDpsHHVKYRSQ4l+DytWwc33RR+5Hr9+jCM7xlnQKsKPDdSmUYkWdRNMg+pFJx9NnTuDNdfD337\nxh1Rfj79NAw+NmtW+F9E4qNukjGZOxeOPx5GjYILL4THH6/85A6hf/6XvxzGxRGRyqcE3wSrV8PY\nsaEP+777hm6Pxx1Xnt0et5TKNCLJoQSfA3e4557Qu2TWrND18aKLoH37uCMrvCOOgCeeCF9mIlLZ\nKvBSYGlNmRLq7CtWwJ13wqBBcUdUXJ07w+c/D5Mnh141IlK5cjqCN7MxZjbdzKaZ2Z1m1tbMfmZm\nr5jZFDN72MwS9ZtAS5bAaaeFI9pvfQteein5yb2OyjQiyZA1wZtZd+BMoJ+79yUc9Z8AXOnu+7n7\n/sCDwNiiRloia9fCddeFu1Dbtw919lNPbV6DcA0bFn7laf36uCMRkXzkWqJpCXQ0sw1AB2Chu69M\nm98R2FDo4Ert0UfDXag77RR+WWmvveKOKB477xy6ST7/PAwcGHc0IrKlsiZ4d19oZuOAecBq4FF3\nnwxgZpcCJwHLgC8VM9BimjMnjPL42mtwzTVwzDHJ6hmzJYYNC2UaJXiRypX1RiczqwLuB44HlgMT\ngPvc/a60Zc4D2rv7xfWs7yeffDLV1dUAVFVVUVNTQ21tLQCpVAoglvaKFXDaaSkefBAuuKCWMWPg\nuefii6ec2h071nLSSXDTTeURj9pqJ72dSqUYP348ANXV1VxyySV53+iUS4I/Dhji7qdE7ROB/u5+\nRtoyOwEPufu+9axfdneybtgAf/oTXHBB+Lm8X/4yjIcum2zYEEpVTzwBu+8edzQizU+p7mSdBwww\ns3ZmZsBgYIaZ7Zq2zLHAjHwCKZUXXghlh1//Gu6/H26/Xcm9Pi1ahFKVBh8TqVxZE7y7v0Aoy0wB\nXokm3wxcHnWbnAocBpxdtCgLYNEiGD06jPR4+unw73/DgAFxR1Xe1F1SpLIlfrCxTz8Nv396xRXw\nne/AT34CnTrFFk5FqRt8bPZs6No17mhEmhcNNtYI99CXe5994Kmnwu+NXnGFkntTtG0brlFo8DGR\nypTIBD9zJhx5JPzwh3DDDTBxIuy2W9xRVSaVaUQqV6IS/PLloT/7oEFh2NtXX4WhQ+OOqrIdeWQY\nDvm//407EhFpqkQk+PXr4dZbw2iPK1aEG5bGjIHWreOOrPJttx3svz/8859xRyIiTVXxo0n+619w\n1llh3JhJk8JY7VJYdWWao4+OOxIRaYqK7UUzfz786Efw9NPh4umIERpeoFjmzIGDD4aFC0P/eBEp\nvmbZi+aTT+DSS6GmBnr3DhdUR45Uci+m3r2hS5dwk5iIVI6KSfDu8Je/hBEep0yBF1+En/8cOnaM\nO7LmQb1pRCpPRZRoXn01DOP7/vtw/fVw6KEFCk5y9vzz8O1vhwvYIlJ8iS/RLF0KZ5wBgwfDV74S\njtyV3ONx4IHw0Ufw5ptxRyIiuSrLBL9uHdx4I/TpE0ozM2aERN+q4vv8VK66wcdUphGpHGWX4FMp\n6NcP7rsPHnsMfvOb0Bdb4jdsmEaXFKkkZVODf+cdOPfc8OPWV18NX/2qesaUm08+CYOPzZkTetWI\nSPEkoga/ahVcdFG4QalvX3j9dfja15Tcy1G7duF6iAYfE6kMsSV4d7jnnlBnnz0bpk6FCy8Md6RK\n+Ro+XGUakUoRS4lmypQwvMCqVaHb46BBRQ1BCuiDD8KNT4sXhyN6ESmOiivRLFkCp54KRxwBJ50U\nblZScq8sXbrAfvuFESZFpLyVJMGvXQvXXRfuQu3YMQwvcMop0LJlKfYuhaa7WkUqQ0lKNHvu6fTs\nGZJ8nz5F3Z2UwOzZcMghsGCBBh8TKZaSlWjMbIyZTY9+ZPtOM2trZlea2Qwzm2pm95tZgz+Gd+WV\n8PDDSu5JsdtusO22oUuriJSvrAnezLoDZwL93L0vYQz5E4BHgb3dvQaYDVzQ0DaOOUbdHpNGZRqR\n8pfrCXZLoKOZtQI6AAvdfbK7b4jm/xvoUYwApTwNG6YEL1LusiZ4d18IjAPmAQuAZe4+OWOxbwP/\nKHx4Uq769w9dJufMiTsSEWlI1uG7zKwKGA70ApYDE8xspLvfFc3/CbC2rl2fUaNGUV1dDUBVVRU1\nNTXU1tYCkEqlANSuwPYxx8A116Q4/vjyiEdttSu5nUqlGD9+PMDGfJmvrL1ozOw4YIi7nxK1TwT6\nu/sZZjYKOAU41N0/bWD9ovxkn8Rv4kQYNy4MECcihVWqXjTzgAFm1s7MDBgMzDCzocC5wLCGkrsk\n22GHhbuSP/ww7khEpD651OBfACYAU4BXosk3AzcAWwGPmdnLZnZj0aKUstS+ffgBloceijsSEalP\n2QwXLJXpD38ICf6+++KORCRZClGiUYKXvCxZEm58WrwY2raNOxqR5Ki4wcYkebp2hX320eBjIuVI\nCV7ypjHiRcqTSjSStzfegC99Cd59V4OPiRSKSjRSFnbfHbbeGv7zn7gjEZF0SvBSECrTiJQfJXgp\nCI0uKVJ+lOClIPr3D10l33477khEpI4SvBREy5Zw9NEq04iUEyV4KRiNES9SXtRNUgpm9Wro1g3m\nzg0/6SciW07dJKWsdOgQ+sNr8DGR8qAELwWlMo1I+VCJRgpq8WLYYw8NPiaSL5VopOxsvz3svbd+\n5UmkHCjBS8ENG6bukiLlQCUaKbiZM+Hww2HePLC8TjBFmi+VaKQs7bFH+Dm/l1+OOxKR5k0JXgrO\nTIOPiZSDnBK8mY0xs+lmNs3M7jSztmZ2XDRtvZn1K3agUlk0+JhI/LImeDPrDpwJ9HP3vkAr4ATg\nVeArwJNFjVAq0he+AAsWwDvvxB2JSPOVa4mmJdDRzFoBHYCF7j7L3WcDuowmn1E3+NjEiXFHItJ8\nZU3w7r4QGAfMAxYAy9x9crEDk8qnMo1IvFplW8DMqoDhQC9gOTDBzEa6+1257mTUqFFUV1cDUFVV\nRU1NDbW1tQCkojti1E5e+/DDYeTIFJMmwdFHxx+P2mqXczuVSjF+/HiAjfkyX1n7wZvZccAQdz8l\nap8I9Hf3M6L2E8A57l5vpzj1g2/ejjkGRo6EESPijkSkspSqH/w8YICZtTMzAwYDMzJjyScISS6V\naUTik9OdrGY2FvgGsBaYAnwXOAq4AegCLAOmuvsR9ayrI/hmbNEi6NMnDD7Wpk3c0YhUjkIcwWuo\nAim6L3wBfvazMHyBiORGQxVIRdAY8SLx0BG8FN3rr8PQoeGn/DT4mEhudAQvFaFPn/DjH1Onxh2J\nSPOiBC9FZ6YyjUgclOClJDS6pEjpqQYvJbFuHXTrFsaI79kz7mhEyp9q8FIxWrWCo47S4GMipaQE\nLyWju1pFSkslGimZlSuhe3d4913YZpu4oxEpbyrRSEXZaisYNAgefjjuSESaByV4KSmVaURKRyUa\nKamFC2GffcLgY61bxx2NSPlSiUYqTvfusNtu8NRTcUciknxK8FJyKtOIlIYSvJRc3bAFqtyJFJcS\nvJTc3nuHG5+mTYs7EpFkU4KXktPgYyKloQQvsdDgYyLFp26SEot162D77eGVV6BHj7ijESk/Jesm\naWZjzGy6mU0zszvNrI2ZbWtmj5rZLDN7xMx087nkrFUrOPJIHcWLFFPWBG9m3YEzgX7u3hdoBYwA\nzgcmu/sewOPABcUMVJJHZRqR4sq1Bt8S6GhmrYD2wAJgOHB7NP924NjChydJNmQIPPssfPxx3JGI\nJFPWBO/uC4FxwDxCYl/u7pOB7d19cbTMIuBzxQxUkmfrreHggzX4mEixtMq2gJlVEY7WewHLgfvM\n7JtA5pXTBq+kjho1iurqagCqqqqoqamhtrYWgFQqBaB2M2336ZPillvg618vj3jUVjuudiqVYvz4\n8QAb82W+svaiMbPjgCHufkrUPhEYABwK1Lr7YjPrBjzh7n3qWV+9aKRBCxbAvvtq8DGRTKXqRTMP\nGGBm7czMgMHA68ADwKhomZMB3bYiTbbjjtC7NzzzTNyRiCRPLjX4F4AJwBTgFcCAm4ErgMPNbBYh\n6V9exDglwTT4mEhx6EYnid20aSHJv/VWGMZARDQevCTEvvuG/6dPjzcOkaRRgpfYmalMI1IMSvBS\nFjS6pEjhqQYvZWHt2jD42Kuvhp41Is2davCSGK1bwxFHwMSJcUcikhxK8FI2VIcXKSyVaKRsfPxx\nKM8sXBjGqRFpzlSikUTp1AkGDoRHHok7EpFkUIKXsqIx4kUKRyUaKSvz50NNDSxaFH71SaS5UolG\nEqdHD+jVS4OPiRSCEryUHZVpRApDCV7KTl13SVX2RPKjBC9lp29fWL8eXnst7khEKpsSvJQdszA2\njco0IvlRgpeypMHHRPKnbpJSluoGH3vtNdhhh7ijESk9dZOUxGrdGoYO1eBjIvlQgpeypTKNSH6y\nlmjMbHfgXsAJP7i9C3AhkAJ+C3QE3gG+6e4r61lfJRrZIsuXw047hcHHttoq7mhESqskJRp3f8Pd\n93f3fsABwCrgr8CtwI/cfb+o/aN8AhHJtM02MGAAPPpo3JGIVKamlmgOA+a4+7vA7u5ed0P5ZOBr\nBY1MBJVpRPLR1AR/AnBX9Hi6mQ2LHn8d6FGwqEQiw4bBgw/CunVxRyJSeXIer8/MWgPDgPOjSd8G\nbjCzC4EHgDUNrTtq1Ciqq6sBqKqqoqamhtraWgBSqRSA2mrX237rrRRVVfDss7Ucckj88aitdrHa\nqVSK8ePHA2zMl/nKuR98dLT+v+4+tJ55uwF3uPuAeubpIqvk5eKLYcUKGDcu7khESqfU/eBHAHen\n7bxr9H8L4KeEHjUiBafBx0S2TE4J3sw6EC6w/iVt8ggzmwW8Dixw9/GFD08k/ADImjUwY0bckYhU\nFg1VIBXhjDPCj4Gcf372ZUWSQEMVSLNRV6YRkdzpCF4qwpo1YfCxGTOgW7e4oxEpPh3BS7PRpg0M\nGQKTJsUdiUjlUIKXiqG7WkWaRiUaqRjLlkHPnvDee9CxY9zRiBSXSjTSrFRVwUEHwWOPxR2JSGVQ\ngpeKojKNSO5UopGKMncuHHhgKNO0bBl3NCLFoxKNNDu9ekH37vDcc3FHIlL+lOCl4qhMI5IbJXip\nOMOHwwMPxB2FSPlTgpeK068frFoFM2fGHYlIeVOCl4pjpjKNSC6U4KUiqUwjkp26SUpF+vTTMPjY\nrFnhf5HI7SCGAAAFLElEQVSkUTdJabbatoUvf1mDj4k0RgleKpbKNCKNU4lGKtbSpVBdDYsWQYcO\ncUcjUlglKdGY2e5mNsXMXo7+X25mZ5nZfmb2XDTtBTP7fD6BiDRV587w+c/D5MlxRyJSnrImeHd/\nw933d/d+wAHAKuCvwJXAWHffHxgLXFXUSEXqoe6SIg1rag3+MGCOu78LbAC2iaZXAQsKGZhILoYP\nDxda16+POxKR8tOqicufANwdPR4DPGJm4wADBhYyMJFc7Lxz6Cb5/PMwUJ9Akc3kfARvZq2BYcB9\n0aTvAWe7e09Csv994cMTyU5lGpH6NeUI/gjgP+6+JGqf7O5nA7j7BDO7raEVR40aRXV1NQBVVVXU\n1NRQW1sLQCqVAlBb7S1u9+gB111XyxVXlEc8aqu9Je1UKsX48eMBNubLfOXcTdLM7gYedvfbo/Zr\nwP+6+5NmNhi43N0PrGc9dZOUotqwAXbaCR5/HPbYI+5oRAqjEN0kc0rwZtYBmAvs4u4romkDgV8B\nLYFPCMl+Sj3rKsFL0Z1+OvTuDeeeG3ckIoVRsgSf1w6U4KUE/vEPuOwyePrpuCMRKQwleJHIJ5+E\n3jRvvgldu8YdjUj+NNiYSKRdOzj8cA0+JpJOCV4SQ4OPiWxOJRpJjA8/hF12CYOPtW8fdzQi+VGJ\nRiTNdtvB/vtr8DGROkrwkigq04hsohKNJMqcOXDwwbBwIbTQ4YtUMJVoRDL07g1duoTBx0SaOyV4\nSRyVaUQCJXhJHI0uKRIowUviHHggfPQRzJ4ddyQi8VKCl8Rp0QKOOUZlGhEleEmk4cNVphFRN0lJ\npLrBx+bMCb1qRCqNukmKNKBdOxg3DtasiTsSkfjoCF5EpAzpCF5ERBqkBC8iklBK8CIiCdUq2wJm\ntjtwL+CAAbsAFwIDgT2i6dsCH7l7v+KFKiIiTdGki6xm1gKYD/R393fTpl8NLHP3S+tZRxdZRUSa\nKI6LrIcBc9KTe+TrwN35BCKlk0ql4g6hZCrlucYdZ6n2X8z9FHLbhdhW3O8pND3Bn0BGIjezQcAi\nd59TsKikqMrhg1cqlfJc445TCb7w24r7PYUmlGjMrDWwENjL3ZekTb8RmO3u1zawnuozIiJbIN8S\nTdaLrGmOAP6TkdxbAl8FGry4mm+AIiKyZZpSohnBZ+vshwMz3H1h4UISEZFCyCnBm1kHwgXWv2TM\n+kxNXkREykPRx6IREZF46E5WEZGEUoIXEUmopvSiKRgz2xM4G9gOeNzdfxtHHCIilcTMhgNHAVsD\nv3f3xxpdPs4avJkZcLu7nxRbECIiFcbMqoCr3P2UxpbLq0RjZreZ2WIzm5YxfaiZzTSzN8zsvAbW\nPQaYBDyUTwwiIpUmn9wZ+Snwm2z7ybcG/wdgSEaALYBfR9P3BkZEJRnM7EQzu8bMdnD3ie5+FPCt\nPGMQEak0W5o7u5vZ5cBD7j41207yqsG7+zNm1itj8kGEoQvmRoHdAwwHZrr7HcAdZvY/ZnY+0BZ4\nMJ8YREQqTR6580xgMNDJzHZ195sb208xLrLuCKSPNjk/Cnwjd38SeLII+xYRqVS55M4bgBty3aC6\nSYqIJFQxEvwCoGdau0c0TUREGlbw3FmIBG/RvzovAruaWS8zawN8A3igAPsREUmSoufOfLtJ3gU8\nC+xuZvPMbLS7rwfOBB4FXgPucfcZ+exHRCRJSpU7NdiYiEhC6SKriEhCKcGLiCSUEryISEIpwYuI\nJJQSvIhIQinBi4gklBK8iEhCKcGLiCTU/wPD+uVdA10urQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13da85b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plot accuracy scores for beta values\n",
    "'''\n",
    "plt.semilogx(beta_values, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1 NN layer)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set up TF graph\n",
    "'''\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_constant = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # First hidden layer\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    hidden_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    \n",
    "    # Final layer\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(hidden_1, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \\\n",
    "                + beta_constant * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    validation_prediction = tf.nn.softmax(tf.matmul(layer1_validation, weights) + biases)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 402.550934\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 100: 31.101215\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.3%\n",
      "Minibatch loss at step 200: 30.791746\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 62.3%\n",
      "Test accuracy: 68.2%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run TF graph using a restrictive constraint of just 3 batches\n",
    "'''\n",
    "num_steps = 300\n",
    "num_batches = 2\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        \n",
    "        \n",
    "        offset = step % num_batches\n",
    "        \n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels, \n",
    "                     beta_constant : 1e-4}\n",
    "            \n",
    "        _, l, predictions = session.run([optimizer, \n",
    "                                         loss, \n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(validation_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Minibatch accuracy: 100.0% ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set up 1 layer NN with dropout during training\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset= tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    # Hidden layer\n",
    "    # truncated_normal(shape) where shape is the shape of the output tensor\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    # tf.zeros([3, 4], tf.int32) ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes])) \n",
    "    # Output layer\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "    # Training\n",
    "    layer_1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    #############################################\n",
    "    dropout = tf.nn.dropout(layer_1_train, 0.5)\n",
    "    #############################################\n",
    "    logits = tf.matmul(dropout, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    layer_1_validation_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    validation_prediction = tf.nn.softmax(tf.matmul(layer_1_validation_prediction, weights) + biases)\n",
    "    \n",
    "    layer_1_testing_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    testing_prediction = tf.nn.softmax(tf.matmul(layer_1_testing_prediction, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized.\n",
      "Minibatch loss at step 0: 497.118408\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 23.2%\n",
      "Minibatch loss at step 100: 46.612938\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 200: 166.212311\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 300: 25.870216\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 400: 42.795425\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 33.458065\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 600: 16.348604\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 700: 37.543747\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 800: 18.191017\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 87.1%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run NN Graph\n",
    "'''\n",
    "num_steps = 900\n",
    "# num_batches = 5\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized.\")\n",
    "    for step in xrange(num_steps):\n",
    "        # Overfitting offset\n",
    "#         offset = step % num_batches\n",
    "        # Non overfitting offset\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate mini batch\n",
    "        batch_data = train_dataset[offset : (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        # Dictionary to feed to mini batch\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                     tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, \n",
    "                                         loss, \n",
    "                                         train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(validation_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(testing_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set up 3 layer NN \n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "num_hidden_nodes_2 = 256\n",
    "num_hidden_nodes_3 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset= tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Variables\n",
    "    # Hidden layer 1\n",
    "    # truncated_normal(shape) where shape is the shape of the output tensor\n",
    "    '''\n",
    "    truncated_normal()\n",
    "    The generated values follow a normal distribution with specified mean and standard deviation, \n",
    "    except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "    '''\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_hidden_nodes_2], \n",
    "                                                stddev=np.sqrt(2.0 / num_hidden_nodes)))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_hidden_nodes_2]))\n",
    "    \n",
    "    weights_3 = tf.Variable(tf.truncated_normal([num_hidden_nodes_2, num_hidden_nodes_3], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes_2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([num_hidden_nodes_3]))\n",
    "    \n",
    "        \n",
    "    # Output layer\n",
    "    weights = tf.Variable(tf.truncated_normal([num_hidden_nodes_3, num_labels], \n",
    "                                             stddev=np.sqrt(2.0 / num_hidden_nodes_3)))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    layer_1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    layer_2_train = tf.nn.relu(tf.matmul(layer_1_train, weights_2) + biases_2)\n",
    "    layer_3_train = tf.nn.relu(tf.matmul(layer_2_train, weights_3) + biases_3)\n",
    "    \n",
    "\n",
    "    logits = tf.matmul(layer_3_train, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer\n",
    "    '''\n",
    "    tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)\n",
    "    learning_rate: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.\n",
    "    global_step: A scalar int32 or int64 Tensor or a Python number. Global step to use for the decay computation. \n",
    "                 Must not be negative.\n",
    "    decay_steps: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.\n",
    "    decay_rate: A scalar float32 or float64 Tensor or a Python number. The decay rate.\n",
    "    staircase: Boolean. If True decay the learning rate at discrete intervals\n",
    "    '''\n",
    "    learning_rate = tf.train.exponential_decay(0.5,\n",
    "                                               global_step,\n",
    "                                               3000,\n",
    "                                               0.70,\n",
    "                                               staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, \n",
    "                                                                          global_step = global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    layer_1_validation_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    layer_2_validation_prediction = tf.nn.relu(tf.matmul(layer_1_validation_prediction, weights_2) + biases_2)\n",
    "    layer_3_validation_prediction = tf.nn.relu(tf.matmul(layer_2_validation_prediction, weights_3) + biases_3)\n",
    "    validation_prediction = tf.nn.softmax(tf.matmul(layer_3_validation_prediction, weights) + biases)\n",
    "    \n",
    "    layer_1_testing_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    layer_2_testing_prediction = tf.nn.relu(tf.matmul(layer_1_testing_prediction, weights_2) + biases_2)\n",
    "    layer_3_testing_prediction = tf.nn.relu(tf.matmul(layer_2_testing_prediction, weights_3) + biases_3)\n",
    "    testing_prediction = tf.nn.softmax(tf.matmul(layer_3_testing_prediction, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized.\n",
      "Minibatch loss at step 0: 10.227684\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 2000: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 3000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 4000: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.0%\n",
      "Test accuracy: 10.0%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run NN Graph\n",
    "'''\n",
    "num_steps = 5000\n",
    "# num_batches = 5\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized.\")\n",
    "    for step in xrange(num_steps):\n",
    "        # Overfitting offset\n",
    "#         offset = step % num_batches\n",
    "        # Non overfitting offset\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate mini batch\n",
    "        batch_data = train_dataset[offset : (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        # Dictionary to feed to mini batch\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                     tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, \n",
    "                                         loss, \n",
    "                                         train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(validation_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(testing_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
